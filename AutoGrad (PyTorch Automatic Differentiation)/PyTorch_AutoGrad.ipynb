{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Manually  calculating gradient"
      ],
      "metadata": {
        "id": "v-0ee2B8YDej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## e.g if the input is y = x**2\n",
        "def dy_dx(x):\n",
        "  return 2*x"
      ],
      "metadata": {
        "id": "Pkv8S7tYYF6e"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dy_dx(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVNaxs4UYNGt",
        "outputId": "35eeeafb-e357-4404-c043-c079c8adcdee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## calculating gradient of more complex input"
      ],
      "metadata": {
        "id": "Lv9iXpQGagos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## e.g if the inputs are y = x**2 and z = sin(y)\n",
        "\n",
        "import math\n",
        "def  dz_dx(x):\n",
        "  return 2* x * math.cos(x**2)"
      ],
      "metadata": {
        "id": "rAoVatNHafbw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dz_dx(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iulredS1a6nx",
        "outputId": "9220ace8-91a3-4d34-bed6-663cf94d366f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-2.6145744834544478"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### now more complexity in inputs\n",
        "\n",
        "### e.g, y = x^2, z = sin(y),  u = e^z\n",
        "\n",
        " ## as the complexity grows taking the derivative of these nested function is impossible,\n",
        " ## that is where PyTorch's AutoGrad comes"
      ],
      "metadata": {
        "id": "-1jKh-sdbWV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now with torch autograd"
      ],
      "metadata": {
        "id": "MjfgyekqYT8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "x =  torch.tensor(3.0, requires_grad=True)  ## tracking the operations...(computation graph)"
      ],
      "metadata": {
        "id": "tRL0pL9FYZZh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = x**2"
      ],
      "metadata": {
        "id": "FBxJYIhAYfKn"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x   ### requires grad true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_lrn7uAZfPr",
        "outputId": "a3670ff2-a260-4982-ab22-15f8bd17a4b5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3., requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y ## it looks back which operations are performed previously like x was raised power by 2 and we got y ...etc...etc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHQ58bO4ZaXz",
        "outputId": "676b69b6-d8ae-4d0b-90e9-62da82be5257"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(9., grad_fn=<PowBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()  ## dy/dx  ,,, all the derivatives got calculated in the backward direction"
      ],
      "metadata": {
        "id": "2kuFBDFMZb0L"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad ## this is derivative"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZTVsC4tZqJe",
        "outputId": "3cda6907-f9d9-4808-cbee-c420b8b00f9b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### now again let's take the example of nested function like y = x^2, z = sin(y)"
      ],
      "metadata": {
        "id": "aPdGG4mAfOZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(3.0, requires_grad=True) ## this is x"
      ],
      "metadata": {
        "id": "xuLvdwA9Zvbt"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = x ** 2  ## this is y"
      ],
      "metadata": {
        "id": "PLr-4Ug4gJA2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = torch.sin(y)  ## this is z"
      ],
      "metadata": {
        "id": "3nBqoA-dgMe5"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)\n",
        "print(y)\n",
        "print(z)\n",
        "\n",
        "## the computation graph will look like:  x --> (sq) --> y --> sin -- z ....forward direction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LP_LS8W1gOLT",
        "outputId": "e2a5552d-ef5a-4f14-9f57-a68f8df77f0d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3., requires_grad=True)\n",
            "tensor(9., grad_fn=<PowBackward0>)\n",
            "tensor(0.4121, grad_fn=<SinBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### now to calculate dz/dx  --- we have to go backward direction via chain rule\n",
        "\n",
        "z.backward()   ## all the derivatives got calculated.. via chain-rule\n"
      ],
      "metadata": {
        "id": "8PcatTT6gYuJ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad  ## this is derivative we are looking for dz/dx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ou7AxaghJQC",
        "outputId": "88cb7288-2af5-4e8b-d72a-2a5105a8fa9b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-5.4668)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### in PyTorch we can't calculate derivative of intermediate nodes ...(nodes in between) like:\n",
        "\n",
        "y.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbOxRpHbhUYi",
        "outputId": "5953bd99-7809-4c5e-8221-a4172980a997"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-29-3017738319.py:3: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  y.grad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### now a simple neural network example (this is manual one..no pytorch use)\n",
        "\n",
        "-- we are using scalar (only one input like 6.7)...no vector ...for  demo"
      ],
      "metadata": {
        "id": "gLmeCXPsiIVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## building the input schema for NN\n",
        "\n",
        "import torch\n",
        "\n",
        "## inputs\n",
        "x = torch.tensor(6.7)  ## input feature\n",
        "y = torch.tensor(0.0)  ## this is true label(binary classification)\n",
        "\n",
        "\n",
        "w = torch.tensor(1.0)  ## weight\n",
        "b = torch.tensor(0.0)  ## bias"
      ],
      "metadata": {
        "id": "WV36VGKKh2Mg"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## this is for calculating loss with binary_cross_entropy ....(remember: the input values are scaler...not vector etc)\n",
        "\n",
        "\n",
        "\n",
        "def binary_cross_entropy(prediction, target):\n",
        "  epsilon = 1e-8   ### to prevent log(0)\n",
        "  prediction = torch.clamp(prediction,  epsilon, 1-epsilon) ##  to prevent exploding values during loss computation\n",
        "  return -(target * torch.log(prediction) + (1 - target) * torch.log(1 - prediction))"
      ],
      "metadata": {
        "id": "8YRH9-Swi8y9"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## forward pass\n",
        "z = w * x + b    ## weighted sum (linear part)\n",
        "y_pred = torch.sigmoid(z)   ## predicted  probability\n",
        "\n",
        "\n",
        "# compute binary_cross_entropy loss\n",
        "loss = binary_cross_entropy(y_pred, y)"
      ],
      "metadata": {
        "id": "ZbqIgaV0kNit"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## now we have to calculate derivatives\n",
        "\n",
        "# 1-   dL/d(y_pred): loss with respect to the prediction (y_pred)\n",
        "dloss_dy_pred  = (y_pred - y)/(y_pred*(1-y_pred))\n",
        "\n",
        "# 2-   dy_pred/dz: prediction (y_pred) with respect to z (sigmoid derivative)\n",
        "dy_pred_dz = y_pred *  (1 - y_pred)\n",
        "\n",
        "# 3.   dz/dw and dz/db: z with respect to w and b\n",
        "dz_dw = x   # dz/dw = x\n",
        "dz_db = 1   # dz/db = 1\n",
        "\n",
        "\n",
        "dL_dw = dloss_dy_pred * dy_pred_dz * dz_dw   ## chain rule\n",
        "dL_db = dloss_dy_pred * dy_pred_dz * dz_db   ## chain rule"
      ],
      "metadata": {
        "id": "gFDh8cFVkrsC"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"manually calculated gradient of loss w.r.t weight (dw): {dL_dw}\")\n",
        "print(f\"manually calculated gradient of loss w.r.t bias (db): {dL_db}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cyz9ZBTRqRNt",
        "outputId": "0ac3b18e-71a8-4ff5-c264-e4f4a2b4ee14"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "manually calculated gradient of loss w.r.t weight (dw): 6.691762447357178\n",
            "manually calculated gradient of loss w.r.t bias (db): 0.998770534992218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### now we going to use PyTorch AutoGrad ...(no manual way)\n",
        "\n",
        "-- we are using scalar (only one input like 6.7)...no vector ...for demo"
      ],
      "metadata": {
        "id": "cGTr8dcOqxze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## building the input schema for NN\n",
        "\n",
        "import torch\n",
        "\n",
        "## inputs\n",
        "x = torch.tensor(6.7)  ## input feature\n",
        "y = torch.tensor(0.0)  ## this is true label(binary classification)\n",
        "\n",
        "\n",
        "w = torch.tensor(1.0, requires_grad=True)  ## weight ....requires_grad = True ...cxs we have to calculate gradient with respect to w (weight)\n",
        "b = torch.tensor(0.0, requires_grad=True)  ## bias   ....requires_grad = True ...cxs we have to calculate gradient with respect to b (bias)"
      ],
      "metadata": {
        "id": "sQMuCGvlql1Z"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = w * x + b  ## weighted sum (linear part)\n",
        "z"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U76CNkq2rVf9",
        "outputId": "8c46da58-74f5-4c45-9c17-86b9e980da47"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.7000, grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = torch.sigmoid(z) ## passing z in sigmoid\n",
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfcECiL1sD0R",
        "outputId": "42fe8d7e-f6fa-4fbe-984a-b49b8418245a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9988, grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = binary_cross_entropy(y_pred, y) ## this is the loss\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mc5KIT6csDwC",
        "outputId": "e2f1a1a3-49ea-4e65-ce1f-770cfd15ba54"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.7012, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## forward pass --> the computation graph will look like:  w*x + b --> z --> sigmoid(z) --> y_pred and actual_y--> loss_function --> loss\n",
        "\n",
        "## the backward pass will go calcuting the derivatives... like delta_Loss/delta_y_pred and till the delta_loss/delta_weight etc..."
      ],
      "metadata": {
        "id": "fKWDeW9MsXX9"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward() ## this will calculate all the derivates --chain-rule....w.r.t weight and bias both\n"
      ],
      "metadata": {
        "id": "ey5ILuN5tPNf"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3vlEUY-tfpD",
        "outputId": "8120f2ef-2d39-4877-c032-701ccf40422a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(6.6918)\n",
            "tensor(0.9988)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### It's not we can only do it with single value(scaler), as our main work is with vectors...\n",
        "-- now let's take the another demo example for vector"
      ],
      "metadata": {
        "id": "rpvoTGGduqBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x  = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n"
      ],
      "metadata": {
        "id": "dA_YKLdru3oY"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = (x**2).mean()\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXTigm2JvDB4",
        "outputId": "8a7f109d-7be2-4154-def4-6362a874eae4"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.6667, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()  ## this will calculate derivatives  ...partial differentiation...cxs multiple variables\n"
      ],
      "metadata": {
        "id": "P71c0fusvL-0"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad  ## these are derivatives w.r.t x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Z2VzMZDvSuc",
        "outputId": "3bbe7eca-2eb4-414a-c7ac-9f107f97d6b5"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.6667, 1.3333, 2.0000])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clearing Gradients\n",
        "\n",
        "-- Clearing gradients in PyTorch refers to resetting the gradients of your model's parameters to zero before computing the next backward pass."
      ],
      "metadata": {
        "id": "sb8-8TXvwQIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## for example:\n",
        "\n",
        "x = torch.tensor(2.0, requires_grad=True)  ## input\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0A6hyPXsvX0h",
        "outputId": "90b0bc43-c776-4875-a5b0-8181f344d135"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2., requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = x ** 2\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6e1CR3H2CS_",
        "outputId": "767d4260-3aff-4b30-9d9b-c8a198f9127d"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4., grad_fn=<PowBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward() ## derivative"
      ],
      "metadata": {
        "id": "YgpjDCQT2DpR"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yz8W_fyx2HJ4",
        "outputId": "a64d326c-cc40-433d-c486-d04e7586f24c"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## till now it's normal, but now if I again do the forward pass\n",
        "\n",
        "y = x ** 2"
      ],
      "metadata": {
        "id": "aK3HnVg82QYK"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()  ## again derivative"
      ],
      "metadata": {
        "id": "h6D22aDp2knb"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad   ## the derivative is now 8.0 ... so the new value is added to the previous value...that is the problem"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BA2s1xVV2l41",
        "outputId": "81b1dfe3-faf4-463f-960d-fb004765bdc2"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(8.)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### running forward, backward again and again.. does not clear already calculated gradients"
      ],
      "metadata": {
        "id": "FXv_V5VW2mv_"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## in case of neural network...this is massive problem..cxs we do lot of\n",
        "## forward and backward pass.. so adding gradient's like this ...will never give us exact gradient...(that is problem)"
      ],
      "metadata": {
        "id": "hE7BRZsH274Y"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## the solution is When we are doing multiple passes we have to clear the previous gradient before another forward pass.."
      ],
      "metadata": {
        "id": "Qbl-3cLM3Rbi"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## so the change is ....\n",
        "x.grad.zero_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cqxRuzH3d79",
        "outputId": "b616f32f-4ecf-4f1f-c2fe-f378fef631a2"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## if again I run the forward pass see the results...\n",
        "y = x ** 2"
      ],
      "metadata": {
        "id": "DfExRFJz3kEk"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()   ## calculating derivative this time ...no previous pass values will be aded"
      ],
      "metadata": {
        "id": "9M7z2dNX3rOh"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad  ## this is new fresh gradient..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQxJeSvg3tjh",
        "outputId": "1a930d35-2dac-40a3-ba9b-15d221703109"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to disable gradient tracking...(that computation graph)"
      ],
      "metadata": {
        "id": "i1iVVZi437Ev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## option 1 --- requires_grad_(False)\n",
        "## option 2 --- detach()\n",
        "## option 3 --- torch.no_grad()"
      ],
      "metadata": {
        "id": "o2ybBpW14BXt"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## requires_grad option\n",
        "\n",
        "x = torch.tensor(3.0, requires_grad=False)\n",
        "x   ## no requires_grad..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gf9Y7Pkr47-6",
        "outputId": "9b081b3f-69ec-4256-c5dc-df956da5f4d9"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.)"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = x ** 2"
      ],
      "metadata": {
        "id": "rsdmqGga47vF"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y    ## no grad function...(nothing in autograd computation graph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5HiLV_w5SI3",
        "outputId": "202b0b40-0d08-4e1c-f738-38e7a477167a"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(9.)"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()   ## so we can't compute any gradient"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "bkic1vLm5UDv",
        "outputId": "45548e87-8c41-45f0-e4d0-a9d6c18f54d0"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-74-895339221.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m## so we can't compute any gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## detach option\n",
        "\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2Y1GQ1L5n46",
        "outputId": "27ea6a17-72c8-47eb-9b2b-95bd7311153e"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2., requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = x.detach()  ## this z will contain same values...but with no gradient tracking...\n",
        "z"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-9jQJI253xk",
        "outputId": "5765a5b1-69be-4df2-ae28-ac2903f37a5f"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.)"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## torch no_grad function option\n",
        "\n",
        "x = torch.tensor(4.0, requires_grad=True)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_DcR82z579P",
        "outputId": "f8538126-f2f5-4e37-84f9-e2d3d7698df3"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4., requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():      ## by using with keyword....and calling the function..to stop disable tracking\n",
        "  y = x ** 2"
      ],
      "metadata": {
        "id": "_MEV04rb6iBe"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y   ## no tracking"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ORfwSJT6okh",
        "outputId": "58f26494-9323-4350-af1a-1560ca11988d"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(16.)"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()     ### now it won't work"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "2CX9O4H46thc",
        "outputId": "e7c98b6c-4a4b-4924-d81a-6f41fdff2145"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-80-1283682180.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m### now it won't work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### End ..thanks for sticking till here....\n"
      ],
      "metadata": {
        "id": "YYrJKuCT7CHi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MpB4xzcU7DU4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}